{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8197cf1b",
   "metadata": {},
   "source": [
    "## The Complete Step-by-Step Roadmap I Followed\n",
    "\n",
    "## **1. Data Collection Pipeline** \n",
    "*Building the data foundation - getting raw information from multiple sources*\n",
    "\n",
    "### **1.1 API Integration Setup** (`notebook: 01_data_collection.ipynb`)\n",
    "- **What**: Connect to free data sources and test API calls\n",
    "- **How**: Python `requests` library to pull data from each source\n",
    "- **Tools**: Companies House API, Guardian News API, Twitter API v2, Reddit API\n",
    "- **Why**: Need consistent, automated way to gather fresh data daily\n",
    "\n",
    "### **1.2 Company Data Foundation** (`notebook: 01_data_collection.ipynb`)\n",
    "- **What**: Get basic info for your target companies (FTSE 100 subset)\n",
    "- **How**: Scrape company identifiers, sectors, stock symbols from FTSE website\n",
    "- **Tools**: `BeautifulSoup` for web scraping, `pandas` for data organization\n",
    "- **Why**: Need standardized company list to track consistently across all data sources\n",
    "\n",
    "### **1.3 Multi-Source Data Fetching** (`notebook: 01_data_collection.ipynb`)\n",
    "- **What**: Pull actual data from each source for your company list\n",
    "- **How**: Build separate functions for news articles, social media posts, regulatory filings\n",
    "- **Tools**: `requests`, `tweepy` (Twitter), `praw` (Reddit), Companies House API\n",
    "- **Why**: Each source has different data formats - need unified approach\n",
    "\n",
    "### **1.4 Data Storage System** (`notebook: 01_data_collection.ipynb`)\n",
    "- **What**: Save all collected data in organized, searchable format\n",
    "- **How**: SQLite database with tables for companies, articles, social posts, filings\n",
    "- **Tools**: `sqlite3`, `sqlalchemy` for database management\n",
    "- **Why**: Need to store historical data and avoid re-downloading same content\n",
    "\n",
    "## **2. Data Processing & Cleaning**\n",
    "*Converting messy real-world data into clean, analyzable format*\n",
    "\n",
    "### **2.1 Text Preprocessing** (`notebook: 02_data_processing.ipynb`)\n",
    "- **What**: Clean up text data from all sources (remove ads, formatting, duplicates)\n",
    "- **How**: Remove HTML tags, standardize encoding, filter out promotional content\n",
    "- **Tools**: `BeautifulSoup`, `re` (regex), `pandas` string methods\n",
    "- **Why**: Raw scraped text is messy - models perform better on clean data\n",
    "\n",
    "### **2.2 Duplicate Detection** (`notebook: 02_data_processing.ipynb`)\n",
    "- **What**: Find and remove same stories appearing across multiple sources\n",
    "- **How**: Text similarity comparison using basic hashing and fuzzy matching\n",
    "- **Tools**: `fuzzywuzzy` library for string similarity, `hashlib` for content hashing\n",
    "- **Why**: Same ESG incident gets reported multiple times - avoid double-counting\n",
    "\n",
    "### **2.3 Company Name Matching** (`notebook: 02_data_processing.ipynb`)\n",
    "- **What**: Link articles/posts to correct companies (BP vs BP Plc vs British Petroleum)\n",
    "- **How**: Build company name variations dictionary and matching algorithm\n",
    "- **Tools**: `fuzzywuzzy`, custom matching functions, manual validation lists\n",
    "- **Why**: Companies mentioned inconsistently across sources - need accurate linking\n",
    "\n",
    "### **2.4 Date Standardization** (`notebook: 02_data_processing.ipynb`)\n",
    "- **What**: Convert all timestamps to consistent format across sources\n",
    "- **How**: Parse different date formats and convert to UTC datetime\n",
    "- **Tools**: `dateutil.parser`, `datetime`, timezone handling\n",
    "- **Why**: Need chronological analysis - different sources use different date formats\n",
    "\n",
    "## **3. NLP Risk Classification System**\n",
    "*Teaching computers to identify ESG risks in text*\n",
    "\n",
    "### **3.1 ESG Risk Category Definition** (`notebook: 03_nlp_classification.ipynb`)\n",
    "- **What**: Define specific ESG risk types you want to detect\n",
    "- **How**: Research real ESG incidents and create classification scheme\n",
    "- **Tools**: Manual research, regulatory guidance documents, ESG frameworks\n",
    "- **Why**: Models need clear target categories - \"Environmental Risk\" too vague\n",
    "\n",
    "### **3.2 Zero-Shot Classification Testing** (`notebook: 03_nlp_classification.ipynb`)\n",
    "- **What**: Test pre-trained models on sample ESG articles without training\n",
    "- **How**: Use HuggingFace transformers with predefined risk categories\n",
    "- **Tools**: `transformers` library, `torch`, BART or RoBERTa models\n",
    "- **Why**: See baseline performance before investing in custom training\n",
    "\n",
    "### **3.3 Training Data Creation** (`notebook: 03_nlp_classification.ipynb`)\n",
    "- **What**: Label sample articles with correct ESG risk categories\n",
    "- **How**: Manual annotation of 200-500 articles across your risk types\n",
    "- **Tools**: Simple labeling interface, `pandas` for data management\n",
    "- **Why**: Need labeled examples to train/fine-tune models for better accuracy\n",
    "\n",
    "### **3.4 Model Fine-tuning** (`notebook: 03_nlp_classification.ipynb`)\n",
    "- **What**: Adapt pre-trained financial models to your specific ESG categories\n",
    "- **How**: Fine-tune FinBERT or similar on your labeled training data\n",
    "- **Tools**: `transformers`, `torch`, HuggingFace model hub, GPU if available\n",
    "- **Why**: General models miss financial/ESG nuances - custom training improves accuracy\n",
    "\n",
    "### **3.5 Model Validation** (`notebook: 03_nlp_classification.ipynb`)\n",
    "- **What**: Test model accuracy on new articles it hasn't seen\n",
    "- **How**: Split data into training/testing sets, measure precision/recall\n",
    "- **Tools**: `sklearn.metrics`, confusion matrices, validation techniques\n",
    "- **Why**: Need confidence that model actually works on real data\n",
    "\n",
    "## **4. Risk Scoring Engine**\n",
    "*Converting ESG events into comparable risk scores*\n",
    "\n",
    "### **4.1 Event Severity Scoring** (`notebook: 04_risk_scoring.ipynb`)\n",
    "- **What**: Assign severity scores to different types of ESG incidents\n",
    "- **How**: Research historical incidents and their financial/reputational impact\n",
    "- **Tools**: Historical case studies, regulatory fine databases, stock price analysis\n",
    "- **Why**: Oil spill more severe than missed diversity target - need quantification\n",
    "\n",
    "### **4.2 Source Credibility Weighting** (`notebook: 04_risk_scoring.ipynb`)\n",
    "- **What**: Weight information differently based on source reliability\n",
    "- **How**: Assign credibility scores to news outlets, social media accounts, official filings\n",
    "- **Tools**: Media bias databases, follower counts, verification status, manual research\n",
    "- **Why**: Financial Times more reliable than random Twitter account - factor this in\n",
    "\n",
    "### **4.3 Temporal Decay Modeling** (`notebook: 04_risk_scoring.ipynb`)\n",
    "- **What**: Make recent events count more than old ones in current risk assessment\n",
    "- **How**: Apply mathematical decay function so risk decreases over time\n",
    "- **Tools**: Exponential decay formulas, `numpy` for calculations\n",
    "- **Why**: 2-year-old controversy less relevant than last week's incident\n",
    "\n",
    "### **4.4 Composite Risk Score Algorithm** (`notebook: 04_risk_scoring.ipynb`)\n",
    "- **What**: Combine severity, credibility, and recency into single risk score (0-100)\n",
    "- **How**: Weighted formula incorporating all factors with normalization\n",
    "- **Tools**: Custom scoring functions, statistical normalization techniques\n",
    "- **Why**: Users need single number for quick decision-making and comparison\n",
    "\n",
    "## **5. AI Agent Development**\n",
    "*Building autonomous research and prediction capabilities*\n",
    "\n",
    "### **5.1 Local LLM Setup** (`notebook: 05_ai_agents.ipynb`)\n",
    "- **What**: Install and test local language model for agent reasoning\n",
    "- **How**: Download and run Ollama with Llama 3.1 or similar model\n",
    "- **Tools**: Ollama, local GPU setup, or CPU-based inference\n",
    "- **Why**: Need reasoning capabilities for agents without API costs\n",
    "\n",
    "### **5.2 Research Agent Framework** (`notebook: 05_ai_agents.ipynb`)\n",
    "- **What**: Build agent that automatically investigates ESG risk spikes\n",
    "- **How**: LLM-powered agent that can query databases and search for context\n",
    "- **Tools**: LangChain or custom agent framework, local LLM integration\n",
    "- **Why**: When risk spike detected, need automatic deeper investigation\n",
    "\n",
    "### **5.3 Agent-Database Integration** (`notebook: 05_ai_agents.ipynb`)\n",
    "- **What**: Let agents query your data storage to find relevant information\n",
    "- **How**: Build tools agents can use to search companies, articles, historical data\n",
    "- **Tools**: SQLite query functions, agent tool interfaces\n",
    "- **Why**: Agents need access to your collected data to provide insights\n",
    "\n",
    "### **5.4 Predictive Alert Agent** (`notebook: 05_ai_agents.ipynb`)\n",
    "- **What**: Agent that analyzes patterns to predict future risk developments\n",
    "- **How**: Time series analysis combined with LLM pattern recognition\n",
    "- **Tools**: Statistical forecasting, LLM reasoning, historical pattern matching\n",
    "- **Why**: Early warning more valuable than just reactive monitoring\n",
    "\n",
    "## **6. Dashboard Development**\n",
    "*Creating user interface for consultants to interact with the system*\n",
    "\n",
    "### **6.1 Web Framework Setup** (`notebook: 06_dashboard.ipynb`)\n",
    "- **What**: Choose and set up web framework for interactive dashboard\n",
    "- **How**: Install Streamlit or Dash for rapid web app development\n",
    "- **Tools**: `streamlit` or `plotly dash`, basic HTML/CSS knowledge\n",
    "- **Why**: Need web interface consultants can use - Jupyter notebooks not professional\n",
    "\n",
    "### **6.2 Company Search Interface** (`notebook: 06_dashboard.ipynb`)\n",
    "- **What**: Build search functionality to find and select companies\n",
    "- **How**: Search bar with autocomplete, dropdown menus for sectors\n",
    "- **Tools**: Streamlit widgets, database queries\n",
    "- **Why**: Users need easy way to navigate your company database\n",
    "\n",
    "### **6.3 Risk Visualization Components** (`notebook: 06_dashboard.ipynb`)\n",
    "- **What**: Create charts showing risk scores, trends, and breakdowns\n",
    "- **How**: Interactive plots using Plotly for risk timelines and category breakdowns\n",
    "- **Tools**: `plotly`, `matplotlib`, dashboard charting libraries\n",
    "- **Why**: Visual presentation much more effective than raw numbers\n",
    "\n",
    "### **6.4 Alert System Interface** (`notebook: 06_dashboard.ipynb`)\n",
    "- **What**: Display recent alerts and allow users to set notification preferences\n",
    "- **How**: Alert panels, email integration for notifications\n",
    "- **Tools**: Email libraries for notifications, alert scheduling\n",
    "- **Why**: Consultants need to be notified of important developments immediately\n",
    "\n",
    "### **6.5 Report Generation** (`notebook: 06_dashboard.ipynb`)\n",
    "- **What**: Auto-generate PDF reports with company ESG risk analysis\n",
    "- **How**: Template-based PDF generation with risk data and visualizations\n",
    "- **Tools**: `reportlab` or `weasyprint` for PDF creation, Jinja2 for templates\n",
    "- **Why**: Consultants need professional documents to share with clients\n",
    "\n",
    "## **7. System Integration & Testing**\n",
    "*Connecting all parts and ensuring reliability*\n",
    "\n",
    "### **7.1 Automated Pipeline Setup** (`notebook: 07_system_integration.ipynb`)\n",
    "- **What**: Schedule automatic daily data collection and processing\n",
    "- **How**: Cron jobs or task scheduler to run data collection scripts\n",
    "- **Tools**: `schedule` library, system cron, or task automation\n",
    "- **Why**: Manual daily data updates not sustainable - need automation\n",
    "\n",
    "### **7.2 Error Handling & Monitoring** (`notebook: 07_system_integration.ipynb`)\n",
    "- **What**: Add robust error handling for API failures, data issues\n",
    "- **How**: Try-catch blocks, logging, fallback procedures for failed API calls\n",
    "- **Tools**: `logging` library, error notification systems\n",
    "- **Why**: APIs fail, websites change - system needs to handle problems gracefully\n",
    "\n",
    "### **7.3 Performance Testing** (`notebook: 07_system_integration.ipynb`)\n",
    "- **What**: Test system performance with full data load and multiple users\n",
    "- **How**: Load testing, memory usage monitoring, speed optimization\n",
    "- **Tools**: Performance profiling tools, database optimization\n",
    "- **Why**: Need to ensure system works at scale with real usage patterns\n",
    "\n",
    "### **7.4 Historical Validation** (`notebook: 07_system_integration.ipynb`)\n",
    "- **What**: Test system against known historical ESG incidents\n",
    "- **How**: Run system on historical data and verify it would have caught major incidents\n",
    "- **Tools**: Historical data replay, validation metrics\n",
    "- **Why**: Prove system works by showing it would have predicted past events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715076b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported without issues\n"
     ]
    }
   ],
   "source": [
    "import requests # I'll be working a lot with API's \n",
    "import pandas as pd  \n",
    "from datetime import datetime  \n",
    "import json  # Something to help read the API responses \n",
    "\n",
    "print(\"Libraries imported without issues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd3d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet connection working\n",
      "   Response time: 1.78 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try to connect to a simple, reliable website\n",
    "    response = requests.get(\"https://httpbin.org/status/200\", timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Internet connection working\")\n",
    "        print(f\"   Response time: {response.elapsed.total_seconds():.2f} seconds\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Connection issue: Got status code {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"ERROR: Cannot connect to internet\")\n",
    "    print(f\"   Error details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6db2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Companies House API\n",
      "FAILED: Got status code 401\n",
      "\n",
      "Try troubleshooting the setup\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Testing Companies House API\")\n",
    "\n",
    "def test_companies_house():\n",
    "    # Testing with good old Tesco and its wonderful meal deals\n",
    "    company_number = \"00102498\"  # Tesco's official UK company number\n",
    "    url = f\"https://api.company-information.service.gov.uk/company/{company_number}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Convert the response to readable format\n",
    "            company_data = response.json()\n",
    "            \n",
    "            # Extract key information\n",
    "            name = company_data.get('company_name', 'Unknown')\n",
    "            status = company_data.get('company_status', 'Unknown')\n",
    "            company_type = company_data.get('type', 'Unknown')\n",
    "            \n",
    "            print(\"Good to go with the CH API\")\n",
    "            print(f\"   Company: {name}\")\n",
    "            print(f\"   Status: {status}\")\n",
    "            print(f\"   Type: {company_type}\")\n",
    "            print(\"Sample data looks fine\")\n",
    "            \n",
    "            return True, company_data\n",
    "            \n",
    "        else:\n",
    "            print(f\"FAILED: Got status code {response.status_code}\")\n",
    "            return False, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Run the test\n",
    "success, data = test_companies_house()\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nTime to get a sizeable list of companies \")\n",
    "else:\n",
    "    print(f\"\\nTry troubleshooting the setup\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
